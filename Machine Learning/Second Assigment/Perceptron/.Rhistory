the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
DistanceFromPlane = function(z, w, b) {
sum(z * w) + b
}
ClassifyLinear = function(x, w, b) {
distances = apply(x, 1, DistanceFromPlane, w, b)
return(ifelse(distances < 0, -1, +1))
}
EuclideanNorm <- function(x) {
return(sqrt(sum(x * x)))
}
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
}
}
}
s = EuclideanNorm(w)
return(list(w = w/s, b = b/s, steps = iterations))
}
# very easy
# x2 = x1 + 1/2
set.seed(1)
x1 <- runif(20,-1,1)
x2 <- runif(20,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
the_perceptron <- PerceptronFunction(x,y)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .5)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .01)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
# error
print(the_perceptron)
## Slope value close to one?
the_perceptron$w[1]/-(the_perceptron$w[2])
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
# error
print(the_perceptron)
## Slope value close to one?
the_perceptron$w[1]/(-the_perceptron$w[2])
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
# error
print(the_perceptron)
## Slope value close to one?
the_perceptron$w[1]/(-the_perceptron$w[2])
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
# very easy
# x2 = x1 + 1/2
set.seed(1)
x1 <- runif(50,-1,1)
x2 <- runif(50,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
the_perceptron <- PerceptronFunction(x,y)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
# very easy
# x2 = x1 + 1/2
set.seed(1)
x1 <- runif(20,-1,1) ## only 20 because qe don't want too many of plots
x2 <- runif(20,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
## Modification of previously defined function so diferent lines are plotted
PlotData2 <- function(x, y, b, w) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-5,5), ylim = c(-5,5), cex = 2)
abline(a = b,b = w)
}
PerceptronFunction2 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
PlotData2(x, y, -b/w[2], -w[1]/w[2])
}
}
}
}
PerceptronFunction2(x, y)
# very easy
# x2 = x1 + 1/2
set.seed(1)
x1 <- runif(100,-1,1)
x2 <- runif(100,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
the_perceptron <- PerceptronFunction(x,y)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .5)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .5)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .01)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(the_perceptron)
print(sum(abs(y - predicted_y)))
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
# error
print(the_perceptron)
## Slope value close to one?
the_perceptron$w[1]/(-the_perceptron$w[2])
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
# error
print(the_perceptron)
## Slope value close to one?
the_perceptron$w[1]/(-the_perceptron$w[2])
## Intercept close to 0.5?
the_perceptron$b/(-the_perceptron$w[2])
## Intercept close to 0.5?
round(the_perceptron$b/(-the_perceptron$w[2]),1)
the_perceptron <- PerceptronFunction(x,y, learning.rate = .000000000000000000000000000000000001)
# error
print(the_perceptron)
## Slope value close to one?
round(the_perceptron$w[1]/(-the_perceptron$w[2]),1) == 1
## Intercept close to 0.5?
round(the_perceptron$b/(-the_perceptron$w[2]),1) == 0.5
#---------- load library ----------
if("mlbench" %in% rownames(installed.packages()) == FALSE) {install.packages("mlbench", dependencies = TRUE)}
library('mlbench')
# load data
data(BostonHousing2)
# info about the data
str(BostonHousing2)
summary(BostonHousing2)
# Using cut
BostonHousing2$age_range <- cut(BostonHousing2$age, breaks = c(-Inf, 20, 30, 40, 60, Inf),
labels = c(0, 1, 2, 3, 4),
right = FALSE)
BostonHousing2$age <- NULL
# Using cut
BostonHousing2$age_range <- cut(BostonHousing2$age, breaks = c(-Inf, 20, 30, 40, 60, Inf),
labels = c(0, 1, 2, 3, 4),
right = FALSE)
# Using cut
BostonHousing2$age_range <- cut(BostonHousing2$age, breaks = c(-Inf, 20, 30, 40, 60, Inf),
labels = c(0, 1, 2, 3, 4),
right = FALSE)
# Using cut
BostonHousing2$age_range <- cut(BostonHousing2$age, breaks = c(-Inf, 20, 30, 40, 60, Inf),
labels = c(0, 1, 2, 3, 4),
right = FALSE)
View(BostonHousing2)
BostonHousing2$homes_range <- as.factor(ifelse(BostonHousing2$medv <
((max(BostonHousing2$medv) - min(BostonHousing2$medv))/2), 0, 1))
# delete medv variable
BostonHousing2$medv <- NULL
plot(BostonHousing2$homes_range)
title(main = "Home range", xlab = "Home Range", ylab = "frequency")
#by age
plot(BostonHousing2[BostonHousing2$age_range == 0, ncol(BostonHousing2)])
title(main = "Home Range for under 20", xlab = "Home Range", ylab = "frequency")
ages_na <- which(is.na(BostonHousing2$age_range) == TRUE)
homes_na <- which(is.na(BostonHousing2$homes_range) == TRUE)
# no NAs in this case
# divide into test and training sets
# create new col "train"" and assign 1 or 0 in 80/20 proportion via random uniform dist
BostonHousing2[, "train"] <- ifelse(runif(nrow(BostonHousing2)) < 0.80, 1, 0)
# get col number of train / test indicator column (needed later)
trainColNum <- grep("train", names(BostonHousing2))
# separate training and test sets and remove training column before modeling
trainBostonHousing2 <-BostonHousing2[BostonHousing2$train == 1,-trainColNum]
testBostonHousing2 <- BostonHousing2[BostonHousing2$train == 0,-trainColNum]
if("e1071" %in% rownames(installed.packages()) == FALSE) {install.packages("e1071", dependencies = TRUE)}
library('e1071')
naive_bayes_model <- naiveBayes(homes_range ~., data = trainBostonHousing2)
naive_bayes_model
summary(naive_bayes_model)
str(naive_bayes_model)
naive_bayes_test_predict <- predict(naive_bayes_model, testBostonHousing2[, -ncol(testBostonHousing2)])
#confusion matrix
table(pred = naive_bayes_test_predict, true = testBostonHousing2$homes_range)
#fraction of correct predictions
mean(naive_bayes_test_predict == testBostonHousing2$homes_range)
## Calculating the cost function per iteration
PerceptronFunction3 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
costFunctionPerIteration <- c() # initialize cost function
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc))
}
}
}
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc)) ## once every point is correclty
return(costFunctionPerIteration)                                      ##   classified
}
plot(PerceptronFunction3(x, y), type = "l", ylab = "Cost Function", xlab = "Iteration", ylim = c(-1,max(PerceptronFunction3(x, y))))
abline(h = 0, col = "red")
## Calculating the cost function per iteration
PerceptronFunction3 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
costFunctionPerIteration <- c() # initialize cost function
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != ClassifyLinear(x, w, b)))
}
}
}
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc)) ## once every point is correclty
return(costFunctionPerIteration)                                      ##   classified
}
plot(PerceptronFunction3(x, y), type = "l", ylab = "Cost Function", xlab = "Iteration", ylim = c(-1,max(PerceptronFunction3(x, y))))
abline(h = 0, col = "red")
## Calculating the cost function per iteration
PerceptronFunction3 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
costFunctionPerIteration <- c() # initialize cost function
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != ClassifyLinear(x, w, b)))
}
}
}
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc)) ## once every point is correclty
return(costFunctionPerIteration)                                      ##   classified
}
plot(PerceptronFunction3(x, y), type = "l", ylab = "Cost Function", xlab = "Iteration", ylim = c(-1,max(PerceptronFunction3(x, y))))
abline(h = 0, col = "red")
# very easy
# x2 = x1 + 1/2
set.seed(1)
x1 <- runif(20,-1,1) ## only 20 because qe don't want too many of plots
x2 <- runif(20,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
## Modification of previously defined function so diferent lines are plotted
PlotData2 <- function(x, y, b, w) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-5,5), ylim = c(-5,5), cex = 2)
abline(a = b,b = w)
}
PerceptronFunction2 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
PlotData2(x, y, -b/w[2], -w[1]/w[2])
}
}
}
}
## Calculating the cost function per iteration
PerceptronFunction3 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
costFunctionPerIteration <- c() # initialize cost function
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != ClassifyLinear(x, w, b)))
}
}
}
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc)) ## once every point is correclty
return(costFunctionPerIteration)                                      ##   classified
}
plot(PerceptronFunction3(x, y), type = "l", ylab = "Cost Function", xlab = "Iteration", ylim = c(-1,max(PerceptronFunction3(x, y))))
abline(h = 0, col = "red")
set.seed(1)
x1 <- runif(100,-1,1)
x2 <- runif(100,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
## Calculating the cost function per iteration
PerceptronFunction3 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
costFunctionPerIteration <- c() # initialize cost function
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != ClassifyLinear(x, w, b)))
}
}
}
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc)) ## once every point is correclty
return(costFunctionPerIteration)                                      ##   classified
}
plot(PerceptronFunction3(x, y), type = "l", ylab = "Cost Function", xlab = "Iteration", ylim = c(-1,max(PerceptronFunction3(x, y))))
abline(h = 0, col = "red")
set.seed(1)
x1 <- runif(100,-1,1)
x2 <- runif(100,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
## Calculating the cost function per iteration
PerceptronFunction3 <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
costFunctionPerIteration <- c() # initialize cost function
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2  ## como b es la constante, es la que mas mueve con respecto
iterations <- iterations + 1         ##  al origen
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != ClassifyLinear(x, w, b)))
yc <- ClassifyLinear(x, w, b)
}
}
}
costFunctionPerIteration <- c(costFunctionPerIteration, sum(y != yc)) ## once every point is correclty
return(costFunctionPerIteration)                                      ##   classified
}
plot(PerceptronFunction3(x, y), type = "l", ylab = "Cost Function", xlab = "Iteration", ylim = c(-1,max(PerceptronFunction3(x, y))))
abline(h = 0, col = "red")
