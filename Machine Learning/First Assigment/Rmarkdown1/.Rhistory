}
expected  <- factor(Y)
predicted <- factor(apply(X, 1, predictX))
results <- confusionMatrix(data = predicted, reference = expected)
print(parametersf)
print(results$table)
}
## Method BFGS
confMatMethod <- TestGradientDescent_different_methods(X, Y, optimMethod = "BFGS")
TestGradientDescent_different_methods <- function(iterations = 1200, X, Y, method_optim) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations), method = method_optim)
#set parameters
parameters <- parameters_optimization$par
# Check evolution
return(parameters)
}
matrix_gradientdescent_method_optim <- function(X = X, Y = Y, gradient_method_optim = "BFGS"){
parameters <- TestGradientDescent_different_methods(X = X, Y = Y, method_optim = gradient_method_optim  )
#add_not_add_student: Filter students based on their scores, TestGradientDescent parameters and cut_off.
add_not_add_student  <- function(student_scores, par = parameters, cut_off = 0.5){
added_as_factor <- ifelse(Sigmoid(t(student_scores) %*% parameters) > cut_off, 1, 0)
return(added_as_factor)
}
predicted <- factor(apply(X, 1, add_not_add_student))
expected <- factor(Y)
results <- confusionMatrix(data = predicted, reference = expected)
print( parameters)
print(levels(predicted))
print(levels(expected))
print(results$table)
print( parameters)
}
set.seed(131822)
# METHODS TO CHOOSE:
# "BFGS"
# "CG"
# "L-BFGS-B"
# "SANN"
# "Brent"
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "BFGS")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "SANN")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "CG")
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "L-BFGS-B")'
## problems with convergence
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "Brent")'
## problems with dimension; only work with one dimension
knitr::opts_chunk$set(echo = TRUE)
# At first I create a dataframe in which I will keep the number of iterations and the value of the          # CostFunction
iterations <- data.frame("Iterations" = 0, "CostFunctionValue" = 0)
# Now I do a loop
for (i in seq(1:125)) {
# Here we have the values of the parameters which are calculated using the TestGradientDescent function
parameters <- TestGradientDescent(iterations = i, X = X, Y = Y)
# The convergence value of each iteration calculated using the CostFuction
CostFunctionValue <- CostFunction(parameters, X, Y)
# Now I add the values to the dataframe
iterations[i,] <- c(i, CostFunctionValue)
}
ggplot(iterations,aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_line() + xlab('Iterations') + ylab('CostFunctionValue')
confussion_matrix <- table(data$label, probabilities_of_all_students > 0.85,
dnn = c("Actual", "Predicted"))
# At first I create a dataframe in which I will keep the number of iterations and the value of the          # CostFunction
iterations <- data.frame("Iterations" = 0, "CostFunctionValue" = 0)
# Now I do a loop
for (i in seq(1:125)) {
# Here we have the values of the parameters which are calculated using the TestGradientDescent function
parameters <- TestGradientDescent(iterations = i, X = X, Y = Y)
# The convergence value of each iteration calculated using the CostFuction
CostFunctionValue <- CostFunction(parameters, X, Y)
# Now I add the values to the dataframe
iterations[i,] <- c(i, CostFunctionValue)
}
ggplot() + geom_point(aes(x = 1:100, y = values))
iterations <- data.frame("Iterations" = 0, "CostFunctionValue" = 0)
TestGradientDescent2 <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
convergence <- c(CostFunction(parameters, X, Y))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the “optim” function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
# set parameters
parameters <- parameters_optimization$par
## $par -> 	The best set of parameters found.
# Check evolution
convergence <- c(CostFunction(parameters, X, Y))
return(parameters)
}
for (i in seq(1:100)) {
# Here we have the values of the parameters which are calculated using the TestGradientDescent function
parameters <- TestGradientDescent2(iterations = i, X = X, Y = Y)
# The convergence value of each iteration calculated using the CostFuction
CostFunctionValue <- CostFunction(parameters, X, Y)
# Now I add the values to the dataframe
iterations[i,] <- c(i, CostFunctionValue)
}
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values)) + geom_line(aes(x = iterations$Iterations , y = iterations$CostFunctionValue))
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values)) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue))
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue))
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + xlab("Iteration") + geom_hline(values[nrow(values),2], col = "blue")
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + xlab("Iteration") + geom_hline(values[length(values)], col = "blue")
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + xlab("Iteration") + geom_hline(values[length(values)], color = "blue")
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + xlab("Iteration") + geom_hline(values[length(values)])
iterations <- data.frame("Iterations" = 0, "CostFunctionValue" = 0)
## function has been modifies so it does not print in every iteration
TestGradientDescent2 <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
convergence <- c(CostFunction(parameters, X, Y))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the “optim” function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
# set parameters
parameters <- parameters_optimization$par
## $par -> 	The best set of parameters found.
# Check evolution
convergence <- c(CostFunction(parameters, X, Y))
return(parameters)
}
for (i in seq(1:100)) {
## paremeter per iteration to calculate cost function
## per iteration
iterations[i,] <- c(i, CostFunction(TestGradientDescent2(iterations = i, X = X, Y = Y), X, Y))
}
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + xlab("Iteration") + geom_hline(values[length(values)])
View(iterations)
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_hline(values[length(values)]) + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue))
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_hline(values[length(values)]) "+ geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue))"
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_hline(values[length(values)])
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration")  + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue))
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)])
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "blue")
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "steelblue1") +
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "steelblue") +
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "blue") +
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "blue")
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "steelblue")
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "steelblue", size = 2)
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
###########################################
seqParameters <- data.frame(parameter1 = seq(0,parameters[1], length.out = 100),
parameter2 = seq(0,parameters[2], length.out = 100),
parameter3 = seq(0,parameters[3], length.out = 100))
values <- c()
for (iteration in 1:nrow(seqParameters)) {
values <- c(values, CostFunction(as.double(seqParameters[iteration,]), X, Y))
}
ggplot() + geom_point(aes(x = 1:100, y = values),shape = 21)  + xlab("Iteration") + geom_point(aes(x = iterations$Iterations , y = iterations$CostFunctionValue)) + geom_hline(yintercept =  values[length(values)], color = "steelblue", size = 2) + ggtitle("Convergence to minimum Cost")
TestGradientDescent_different_methods <- function(iterations = 1200, X, Y, method_optim) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations), method = method_optim)
#set parameters
parameters <- parameters_optimization$par
# Check evolution
return(parameters)
}
matrix_gradientdescent_method_optim <- function(X = X, Y = Y, gradient_method_optim = "BFGS"){
parameters <- TestGradientDescent_different_methods(X = X, Y = Y, method_optim = gradient_method_optim  )
#add_not_add_student: Filter students based on their scores, TestGradientDescent parameters and cut_off.
add_not_add_student  <- function(student_scores, par = parameters, cut_off = 0.5){
added_as_factor <- ifelse(Sigmoid(t(student_scores) %*% parameters) > cut_off, 1, 0)
return(added_as_factor)
}
predicted <- factor(apply(X, 1, add_not_add_student))
expected <- factor(Y)
results <- confusionMatrix(data = predicted, reference = expected)
print( parameters)
print(levels(predicted))
print(levels(expected))
print(results$table)
}
set.seed(131822)
# METHODS TO CHOOSE:
# "BFGS"
# "CG"
# "L-BFGS-B"
# "SANN"
# "Brent"
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "BFGS")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "SANN")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "CG")
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "L-BFGS-B")'
## problems with convergence
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "Brent")'
## problems with dimension; only work with one dimension
TestGradientDescent_different_methods <- function(iterations = 1200, X, Y, method_optim) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations), method = method_optim)
#set parameters
parameters <- parameters_optimization$par
# Check evolution
return(parameters)
}
matrix_gradientdescent_method_optim <- function(X = X, Y = Y, gradient_method_optim = "BFGS"){
parameters <- TestGradientDescent_different_methods(X = X, Y = Y, method_optim = gradient_method_optim  )
#add_not_add_student: Filter students based on their scores, TestGradientDescent parameters and cut_off.
add_not_add_student  <- function(student_scores, par = parameters, cut_off = 0.5){
added_as_factor <- ifelse(Sigmoid(t(student_scores) %*% parameters) > cut_off, 1, 0)
return(added_as_factor)
}
predicted <- factor(apply(X, 1, add_not_add_student))
expected <- factor(Y)
results <- confusionMatrix(data = predicted, reference = expected)
print( parameters)
print(levels(predicted))
print(levels(expected))
print(results$table)
}
set.seed(1)
# METHODS TO CHOOSE:
# "BFGS"
# "CG"
# "L-BFGS-B"
# "SANN"
# "Brent"
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "BFGS")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "SANN")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "CG")
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "L-BFGS-B")'
## problems with convergence
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "Brent")'
## problems with dimension; only work with one dimension
TestGradientDescent_different_methods <- function(iterations = 1200, X, Y, method_optim) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations), method = method_optim)
#set parameters
parameters <- parameters_optimization$par
# Check evolution
return(parameters)
}
matrix_gradientdescent_method_optim <- function(X = X, Y = Y, gradient_method_optim = "BFGS"){
parameters <- TestGradientDescent_different_methods(X = X, Y = Y, method_optim = gradient_method_optim  )
#add_not_add_student: Filter students based on their scores, TestGradientDescent parameters and cut_off.
add_not_add_student  <- function(student_scores, par = parameters, cut_off = C){
added_as_factor <- ifelse(Sigmoid(t(student_scores) %*% parameters) > cut_off, 1, 0)
return(added_as_factor)
}
predicted <- factor(apply(X, 1, add_not_add_student))
expected <- factor(Y)
results <- confusionMatrix(data = predicted, reference = expected)
print( parameters)
print(levels(predicted))
print(levels(expected))
print(results$table)
}
set.seed(1)
# METHODS TO CHOOSE:
# "BFGS"
# "CG"
# "L-BFGS-B"
# "SANN"
# "Brent"
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "BFGS")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "SANN")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "CG")
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "L-BFGS-B")'
## problems with convergence
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "Brent")'
## problems with dimension; only work with one dimension
TestGradientDescent_different_methods <- function(iterations = 1200, X, Y, method_optim) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the "optim" function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations), method = method_optim)
#set parameters
parameters <- parameters_optimization$par
# Check evolution
return(parameters)
}
matrix_gradientdescent_method_optim <- function(X = X, Y = Y, gradient_method_optim = "BFGS"){
parameters <- TestGradientDescent_different_methods(X = X, Y = Y, method_optim = gradient_method_optim  )
#add_not_add_student: Filter students based on their scores, TestGradientDescent parameters and cut_off.
add_not_add_student  <- function(student_scores, par = parameters, cut_off = C){
added_as_factor <- ifelse(Sigmoid(t(student_scores) %*% parameters) > cut_off, 1, 0)
return(added_as_factor)
}
predicted <- factor(apply(X, 1, add_not_add_student))
expected <- factor(Y)
results <- confusionMatrix(data = predicted, reference = expected)
print( parameters)
print(levels(predicted))
print(levels(expected))
print(results$table)
}
set.seed(131822)
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "BFGS")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "SANN")
matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "CG")
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "L-BFGS-B")'
## problems with convergence
'matrix_gradientdescent_method_optim(X, Y, gradient_method_optim = "Brent")'
## problems with dimension; only work with one dimension
library(caret)
data <- read.csv("data/4_1_data.csv")
## Cut off
C <- 0.5
## Predictor variables
X <- as.matrix(data[, c(1,2)])
## Add ones to X in the first column (matrix multiplication x b)
X <- cbind(rep(1, nrow(X)), X)
## Response variable
Y <- as.matrix(data$label)
## We want to minimize the cost function. Then derivate this funcion
TestGradientDescent <- function(iterations = 1200, X, Y) {
# Initialize (b, W)
parameters <- rep(0, ncol(X))
# Check evolution
print(paste("Initial Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
# updating (b, W) using gradient update
# Derive theta using gradient descent using optim function
# Look for information about the “optim” function (there are other options)
parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y,
control = list(maxit = iterations))
# set parameters
parameters <- parameters_optimization$par
## $par -> 	The best set of parameters found.
# Check evolution
print(paste("Final Cost Function value: ",
convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
return(parameters)
}
## How to use
parameters <- TestGradientDescent(X = X, Y = Y)
parameters
## probability of admission for student (1 = b, for the calculos)
new_student <- c(1,25,78)
print("Probability of admission for student: ")
print(prob_new_student <- Sigmoid(t(new_student) %*% parameters))
## CONFUSION MATRIX
predictX  <- function(x, par = parameters, cutoff = C){
## Function for predicting the probability of addmiting a student from DF: X
##
return(ifelse(Sigmoid(t(x) %*% parameters) > cutoff, 1, 0))
}
expected  <- factor(Y)
predicted <- factor(apply(X, 1, predictX))
results <- confusionMatrix(data = predicted, reference = expected)
print(results)
![](unregularized.png)
![](images/unregularized.png)
